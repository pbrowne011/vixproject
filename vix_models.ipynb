{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56465d48",
   "metadata": {},
   "source": [
    "# VIX Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4f016",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate three different models of the VIX that we may use in our trading algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacd04b",
   "metadata": {},
   "source": [
    "First, we import the libraries we need for our models. Some are more common (yfinance, pandas, numpy, matplotlib), while others are less so.\n",
    "\n",
    "We import arch.arch_model, where arch is a library that allows us to model the VIX using a GARCH(1, 1) model (see [here](https://bashtage.github.io/arch/univariate/univariate_volatility_modeling.html) and [here](https://bashtage.github.io/arch/univariate/introduction.html) for documentation).\n",
    "\n",
    "We also import the ARIMA model from the [statsmodels library](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMA.html), as well as other functions from [tensorflow](https://keras.io/about/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad1dd8",
   "metadata": {},
   "source": [
    "Next, we download the necessary VIX data using YahooFinance (as Mihai did in the other notebook).\n",
    "\n",
    "Quick note: setting progress to `False` hides the progress bar while the data is downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ec576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VIX data\n",
    "vix = yf.download('^VIX', start='2000-01-01', end='2023-03-21', progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39969fe",
   "metadata": {},
   "source": [
    "We then calculate the log returns of the VIX for our modeling, shifting by 1 to offset the first day of the dataset. The second line drops this part of the table, as it has a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10822afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log returns\n",
    "vix['log_returns'] = np.log(vix['Adj Close'] / vix['Adj Close'].shift(1))\n",
    "vix.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac2e74",
   "metadata": {},
   "source": [
    "### GARCH model\n",
    "Using the arch_model from our arch library, we model the log returns of the VIX, setting error and time dependency terms to order of magnitude of 1. We then use the arch_garch_model.fit() method to fit our model. This produces an output table that is printed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCH/GARCH model\n",
    "arch_garch_model = arch_model(vix['log_returns'], vol='Garch', p=1, q=1)\n",
    "arch_garch_fit = arch_garch_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3456793",
   "metadata": {},
   "source": [
    "### ARIMA model\n",
    "Next, we use the ARIMA library to model the VIX returns, where we set the lag value to 1 for autoregression, have no time differencing, and set our moving averave component order to 1. This means that our model will use the previous value and error term to predict future values of the time series (the VIX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afef80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model\n",
    "arima_model = ARIMA(vix['log_returns'], order=(1, 0, 1))\n",
    "arima_fit = arima_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7fdbc",
   "metadata": {},
   "source": [
    "### Machine Learning that I don't understand\n",
    "\n",
    "**Description from GPT-3.5**: This code prepares data for and trains a GRU (Gated Recurrent Unit) RNN (Recurrent Neural Network) model to forecast future values of the \"log_returns\" column of a pandas DataFrame named \"vix\".\n",
    "\n",
    "The first part of the code prepares the data for the model by reshaping the \"log_returns\" column into a 2D array using the \"reshape\" method, and then scaling the data using the \"MinMaxScaler\" function from the scikit-learn library. This is done to ensure that the data is normalized to a range of 0 to 1, which is a common practice when working with neural networks.\n",
    "\n",
    "The next part of the code defines a function called \"create_dataset\" that creates the input and output data for the time series model. The function takes a dataset and a \"look_back\" parameter, which specifies the number of time steps to look back when creating the input data. The function then creates a set of input and output pairs for the model by iterating over the dataset and selecting a window of \"look_back\" time steps as the input, and the next time step as the output.\n",
    "\n",
    "The code then calls the \"create_dataset\" function to create the input and output data for the model, and reshapes the input data into a 3D array to match the input shape expected by the GRU RNN model.\n",
    "\n",
    "The next part of the code defines the GRU RNN model using the Keras library. The model consists of a GRU layer with 4 units, followed by a Dense layer with a single output unit. The model is then compiled using the mean squared error loss function and the Adam optimizer.\n",
    "\n",
    "Finally, the model is trained using the input and output data created earlier, with a batch size of 1 and a verbose level of 0 to suppress output during training. The model is trained for 100 epochs, which means that the input data is passed through the model 100 times during training, with the weights of the model updated after each pass to minimize the mean squared error loss.\n",
    "\n",
    "**Description from BARD:** The code first prepares the data for the GRU RNN model by scaling it to the range [0, 1] using a MinMaxScaler. Then, it creates a dataset of input and output data by sliding a window of length 1 over the data and taking the first and last elements of each window. The input data is reshaped to (1, 1, look_back) and the GRU RNN model is created. The model is compiled and trained on the data for 100 epochs with a batch size of 1 and a verbose level of 0.\n",
    "\n",
    "The GRU RNN model is a recurrent neural network that uses a gated recurrent unit (GRU) as its basic unit. The GRU has two gates: a reset gate and an update gate. The reset gate decides how much of the previous state to forget, and the update gate decides how much of the new input to keep. The GRU is able to learn long-term dependencies in the data, which makes it well-suited for tasks such as time series forecasting.\n",
    "\n",
    "The code uses the mean squared error (MSE) as the loss function and the Adam optimizer. The Adam optimizer is an adaptive learning rate optimizer that is often used in deep learning. It uses a combination of the first and second moments of the gradients to update the weights of the model.\n",
    "\n",
    "The code trains the model for 100 epochs. An epoch is a single pass through the training data. The batch size is 1, which means that the model is trained on one example at a time. The verbose level is 0, which means that the model does not print any progress reports during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for GRU RNN model\n",
    "train_data = vix['log_returns'].values.reshape(-1, 1)\n",
    "\n",
    "# Scaling the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "# Prepare data for time series model\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        x = dataset[i:(i + look_back), 0]\n",
    "        y = dataset[i + look_back, 0]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train_data_scaled, look_back)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "# GRU RNN model\n",
    "gru_rnn_model = Sequential()\n",
    "gru_rnn_model.add(GRU(4, input_shape=(1, look_back)))\n",
    "gru_rnn_model.add(Dense(1))\n",
    "gru_rnn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "gru_rnn_model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa3ca2",
   "metadata": {},
   "source": [
    "### Print Model Summaries\n",
    "We them print a summary of each of our models, which gives us an indication of their parameters, as well as how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb4ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summaries\n",
    "print(\"ARCH/GARCH Model Summary:\")\n",
    "print(arch_garch_fit.summary())\n",
    "print(\"\\nARIMA Model Summary:\")\n",
    "print(arima_fit.summary())\n",
    "print(\"\\nGRU RNN Model Summary:\")\n",
    "gru_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d9f42",
   "metadata": {},
   "source": [
    "### Example output from first run"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d2391ff",
   "metadata": {},
   "source": [
    "Iteration:      1,   Func. Count:      6,   Neg. LLF: 34614371.312648006\n",
    "Iteration:      2,   Func. Count:     17,   Neg. LLF: 3208564.0948780905\n",
    "Iteration:      3,   Func. Count:     24,   Neg. LLF: 215344941.8532319\n",
    "Iteration:      4,   Func. Count:     36,   Neg. LLF: -6948.554862097307\n",
    "Iteration:      5,   Func. Count:     45,   Neg. LLF: 4841.623415488499\n",
    "Iteration:      6,   Func. Count:     54,   Neg. LLF: -3096.1322117831414\n",
    "Iteration:      7,   Func. Count:     61,   Neg. LLF: 1918905.2383542338\n",
    "Iteration:      8,   Func. Count:     72,   Neg. LLF: 18094.296713341624\n",
    "Iteration:      9,   Func. Count:     81,   Neg. LLF: -7054.450331278178\n",
    "Iteration:     10,   Func. Count:     88,   Neg. LLF: -7092.995492857817\n",
    "Iteration:     11,   Func. Count:     95,   Neg. LLF: -7539.388082739922\n",
    "Iteration:     12,   Func. Count:    101,   Neg. LLF: -7540.0217221942185\n",
    "Iteration:     13,   Func. Count:    106,   Neg. LLF: -7540.021725769191\n",
    "Iteration:     14,   Func. Count:    110,   Neg. LLF: -7540.021725769626\n",
    "Optimization terminated successfully    (Exit mode 0)\n",
    "            Current function value: -7540.021725769191\n",
    "            Iterations: 17\n",
    "            Function evaluations: 110\n",
    "            Gradient evaluations: 14\n",
    "ARCH/GARCH Model Summary:\n",
    "                     Constant Mean - GARCH Model Results                      \n",
    "==============================================================================\n",
    "Dep. Variable:            log_returns   R-squared:                       0.000\n",
    "Mean Model:             Constant Mean   Adj. R-squared:                  0.000\n",
    "Vol Model:                      GARCH   Log-Likelihood:                7540.02\n",
    "Distribution:                  Normal   AIC:                          -15072.0\n",
    "Method:            Maximum Likelihood   BIC:                          -15045.4\n",
    "                                        No. Observations:                 5839\n",
    "Date:                Tue, Mar 21 2023   Df Residuals:                     5838\n",
    "Time:                        10:31:17   Df Model:                            1\n",
    "                                  Mean Model                                  \n",
    "==============================================================================\n",
    "                  coef    std err          t      P>|t|       95.0% Conf. Int.\n",
    "------------------------------------------------------------------------------\n",
    "mu         -4.3683e-04  7.849e-04     -0.557      0.578 [-1.975e-03,1.101e-03]\n",
    "                              Volatility Model                              \n",
    "============================================================================\n",
    "                 coef    std err          t      P>|t|      95.0% Conf. Int.\n",
    "----------------------------------------------------------------------------\n",
    "omega      5.4888e-04  1.621e-04      3.387  7.072e-04 [2.312e-04,8.665e-04]\n",
    "alpha[1]       0.1409  3.010e-02      4.683  2.830e-06   [8.195e-02,  0.200]\n",
    "beta[1]        0.7525  5.460e-02     13.782  3.263e-43     [  0.645,  0.860]\n",
    "============================================================================\n",
    "\n",
    "Covariance estimator: robust\n",
    "\n",
    "ARIMA Model Summary:\n",
    "                               SARIMAX Results                                \n",
    "==============================================================================\n",
    "Dep. Variable:            log_returns   No. Observations:                 5839\n",
    "Model:                 ARIMA(1, 0, 1)   Log Likelihood                7225.896\n",
    "Date:                Tue, 21 Mar 2023   AIC                         -14443.792\n",
    "Time:                        11:16:40   BIC                         -14417.103\n",
    "Sample:                             0   HQIC                        -14434.511\n",
    "                               - 5839                                         \n",
    "Covariance Type:                  opg                                         \n",
    "==============================================================================\n",
    "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const      -6.721e-06      0.001     -0.010      0.992      -0.001       0.001\n",
    "ar.L1          0.7799      0.023     34.308      0.000       0.735       0.825\n",
    "ma.L1         -0.8675      0.018    -48.050      0.000      -0.903      -0.832\n",
    "sigma2         0.0049   4.78e-05    102.971      0.000       0.005       0.005\n",
    "===================================================================================\n",
    "Ljung-Box (L1) (Q):                   1.55   Jarque-Bera (JB):             15030.31\n",
    "Prob(Q):                              0.21   Prob(JB):                         0.00\n",
    "Heteroskedasticity (H):               2.19   Skew:                             1.32\n",
    "Prob(H) (two-sided):                  0.00   Kurtosis:                        10.40\n",
    "===================================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
    "\n",
    "GRU RNN Model Summary:\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " gru (GRU)                   (None, 4)                 84        \n",
    "                                                                 \n",
    " dense (Dense)               (None, 1)                 5         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 89\n",
    "Trainable params: 89\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
